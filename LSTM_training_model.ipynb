{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model for generating text\n",
    "\n",
    "We use keras LSTM cells to build a language model that predicts the next character of text given the input string so far. Use the trained model to make predictions and generate your own favourite author's writing style. The model is trained on the file which is given as input and the weights are saved for further processing.\n",
    "\n",
    "We show samples generated for the case of Famous Five by Enid Blyton. However, just changing the name of the file and providing the file, you can get it to replicate your own favourite authors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EB](https://raw.githubusercontent.com/InFoCusp/ui_resources/master/lstm_deepdive_images/enid_blyton.png)\n",
    "In this example, you train the model on the combined adventures of Famous five, then use the model to compose stuff.\n",
    "The model works surprisingly well in learning the correct words, rules for capitalizing words, full stops and other punctuations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import six\n",
    "import PyPDF2\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gotta print pretty!\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Input: \n",
    "\n",
    "You can change the input path in `input_file` parameter for some different text file.\n",
    "Also, change the name in `input_file_name` - weights will be stored by this name.\n",
    "\n",
    "You use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"input_files/ff.pdf\"\n",
    "input_file_name = 'famous_five'\n",
    "txt =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert PDF files into text file\n",
    "\n",
    "def convert_pdf_into_input(text_source):\n",
    "    txt = ''\n",
    "    file = open(text_source, 'rb')\n",
    "    fileReader = PyPDF2.PdfFileReader(file)\n",
    "    for pageNum in range(fileReader.numPages):\n",
    "        pageObj = fileReader.getPage(pageNum)\n",
    "        txt += pageObj.extractText()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "## If the file format is pdf:\n",
    "if input_file[-4:]=='.pdf':\n",
    "    txt = convert_pdf_into_input(input_file)\n",
    "    \n",
    "## If the file format is .txt:    \n",
    "else: \n",
    "    with tf.gfile.GFile(input_file, 'r') as f:\n",
    "        txt = f.read()\n",
    "txt = re.sub(' +', ' ',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample text from the book: \n",
      "from Daddy - and I only had one from him and Mummy yesterday.' 'I hope it's not bad \n",
      "news,' said George, She would not allow anyone to call her \n",
      "Georgina, and now even the mistresses called her George\n",
      "\n",
      "\n",
      "Full text is 217112 characters long\n"
     ]
    }
   ],
   "source": [
    "# This is our input text (entire file)\n",
    "print('Some sample text from the book: ')\n",
    "print(txt[600:800])\n",
    "print('\\n')\n",
    "print('Full text is {} characters long'.format(len(txt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text into integers (also dropping any non-ascii characters)\n",
    "def transform(txt, pad_to=None):\n",
    "    output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "    if pad_to is not None:\n",
    "        output = output[:pad_to]\n",
    "        output = np.concatenate([\n",
    "            np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
    "            output,\n",
    "        ])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `training_generator` yields (source, target) arrays for training. It first converts the entire text file into integers using `transform` method and removes any non-ascii characters. After that, it randomly chooses `batch_size` number of offsets from the entire text leaving the `seq_len` number of characters from the last. The method then stacks the characters encoded in form of integers, where the current indexed character is the source and the next character is the target for the current character. Thus, we get `batch_size` number of training samples (source and target) of character sequence length as `seq_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator(input_txt,seq_len=100, batch_size=1024):\n",
    "  \n",
    "    tf.logging.info('Input text [%d] %s', len(input_txt), input_txt[:50])\n",
    "    source = transform(input_txt)\n",
    "    #print(len(source))\n",
    "    while True:\n",
    "        offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
    "        # print(offsets)\n",
    "    \n",
    "        # Our model uses sparse crossentropy loss, but Keras requires labels\n",
    "        # to have the same rank as the input logits.  We add an empty final\n",
    "        # dimension to account for this.\n",
    "        yield (\n",
    "            np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
    "            np.expand_dims(\n",
    "                np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]),\n",
    "                -1),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Input text [217112]  Famous Five 02 Five Go Adventuring Again By Enid \n",
      "Batch of 2 Input sequences of length 5\n",
      "array([[ 32,  70, 105, 118, 101],\n",
      "       [114, 115,  32, 119,  97]], dtype=int32)\n",
      "\n",
      "\n",
      "Batch of 2 output sequences of length 5\n",
      "array([[[ 70, 105, 118, 101,  32],\n",
      "        [115,  32, 119,  97, 105]]], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ip, op = six.next(training_generator(txt,seq_len=5, batch_size=2))\n",
    "print(\"Batch of 2 Input sequences of length 5\")\n",
    "pp.pprint(ip)\n",
    "print(\"\\n\")\n",
    "print(\"Batch of 2 output sequences of length 5\")\n",
    "pp.pprint(np.transpose(op,[2,0,1]))\n",
    "\n",
    "# Generates 2 arrays containing 2 lists (input,target) of length 10.  \n",
    "# General shape: `batch_size X seq_len X 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language model: \n",
    "\n",
    "Model to learn and predict the next word given the current word. Here, we are using 2 LSTM layers with 512 nodes in each layer. Here, we give `stateful`=True there is need to maintain state between different sequence strings of input as the model needs to learn the overall semantic and synctatic information as the text is connected. <br>\n",
    "After two LSTM layers, we use `TimeDistributed` to apply a `Dense` layer to each of the timesteps. Since we want output character at each of the input character, we use TimeDistributed layer to wrap a Dense layer at each training and testing input. In this model, the training data generated using `trainig_generator` is used and we take the predicted character as the output at each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
    "    source = tf.keras.Input(\n",
    "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
    "    lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
    "    lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
    "    predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
    "    model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nisarg/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = lstm_model(seq_len=100, batch_size=128, stateful=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Input text [217112]  Famous Five 02 Five Go Adventuring Again By Enid \n",
      "Epoch 1/8\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 4.3805 - sparse_categorical_accuracy: 0.1644\n",
      "Epoch 2/8\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 3.1274 - sparse_categorical_accuracy: 0.1813\n",
      "Epoch 3/8\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 1.8010 - sparse_categorical_accuracy: 0.4799\n",
      "Epoch 4/8\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 1.0378 - sparse_categorical_accuracy: 0.6853\n",
      "Epoch 5/8\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.7772 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 6/8\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.6426 - sparse_categorical_accuracy: 0.8114\n",
      "Epoch 7/8\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.5850 - sparse_categorical_accuracy: 0.8310\n",
      "Epoch 8/8\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.5701 - sparse_categorical_accuracy: 0.8358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf6f28ada0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    training_generator(txt,seq_len=100, batch_size=128),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_path = 'model_weights/' + input_file_name + '_weights.h5'\n",
    "model.save_weights(model_weights_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with the model\n",
    "\n",
    "Use the trained model to make predictions and generate your own novel/play.\n",
    "Start the model off with a *seed* sentence, then generate 250 characters from it. The model makes five predictions from the initial seed.\n",
    "\n",
    "Keras requires the batch size be specified ahead of time for stateful models. We use a sequence length of 1, as we will be feeding in one character at a time and predicting the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "PREDICT_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_txt):\n",
    "    prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "    prediction_model.load_weights(model_weights_path)\n",
    "    seed = transform(seed_txt)\n",
    "    seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "    \n",
    "    # First, run the seed forward to prime the state of the model starting with reset states\n",
    "    prediction_model.reset_states()\n",
    "    for i in range(len(seed_txt) - 1):\n",
    "        prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "    # Now we can accumulate predictions\n",
    "    predictions = [seed[:, -1:]]\n",
    "    for i in range(PREDICT_LEN):\n",
    "        last_word = predictions[-1]\n",
    "        next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "\n",
    "        # sample from our output distribution - to allow variations\n",
    "        next_idx = [         \n",
    "                    np.random.choice(256, p=next_probits[i])\n",
    "                    for i in range(BATCH_SIZE)\n",
    "        ]\n",
    "        predictions.append(np.asarray(next_idx, dtype=np.int32))  \n",
    "\n",
    "    print('Seed text: \"' + seed_txt + '\"\\n')\n",
    "    # Print out the generated text\n",
    "    for i in range(BATCH_SIZE):\n",
    "        print('PREDICTION %d\\n' % i)\n",
    "        p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "        generated = ''.join([chr(c) for c in p])\n",
    "        print(seed_txt + generated[1:])\n",
    "        print()\n",
    "        assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seed the model with our initial string, copied BATCH_SIZE times, which will generate the number of predictions that we want to observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: \"The children, lay back in the soft sand and waited. Soon they heard Juli\"\n",
      "\n",
      "PREDICTION 0\n",
      "\n",
      "The children, lay back in the soft sand and waited. Soon they heard Julian. Hishrent in xd and opened George \n",
      "had gone r, and had a vuy fell have stuck. The firelike Latin,' said Julian. 'I don't really want to,' said Geo\n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "The children, lay back in the soft sand and waited. Soon they heard Julian wind\n",
      " foot his room where have stuck here for \n",
      "th Dick, 'but wentry went on, he was.' 'Ne -this black \n",
      "bus home. \n",
      "They took her. The little girl f\n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "The children, lay back in the soft sand and waited. Soon they heard Julian. 'No, sir,' said Julian, politely, nol explaile - and we need to say about, or solve things. Where's George?' Nobody answering under his breath. '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_txt = 'The children, lay back in the soft sand and waited. Soon they heard Juli'\n",
    "# change seed_text according to our input file\n",
    "generate_text(seed_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: \"The children, lay back in the soft sand and waited. Soon they heard Mr. Rola\"\n",
      "\n",
      "PREDICTION 0\n",
      "\n",
      "The children, lay back in the soft sand and waited. Soon they heard Mr. Roland's deep Tim darkd and spoken help us, as well as how you know ? We are to be collYou\n",
      " suppose all be. Aunte thickly. Julian squeeped cus him. 'Ah, \n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "The children, lay back in the soft sand and waited. Soon they heard Mr. Roland, excitement of his ligttle morning, shall obs you know those big pages he wring up again. 'Pour of the cotnes is she little rudy across to Kirrin \n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "The children, lay back in the soft sand and waited. Soon they heard Mr. Roland did me ? Avery dinners. Th\n",
      "e two artists will be a punishment had not. Seem tou begin lessons again. But I was one here away. Dick put his ar\n",
      "gain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_txt = 'The children, lay back in the soft sand and waited. Soon they heard Mr. Rola'\n",
    "# change seed_text according to our input file\n",
    "generate_text(seed_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "As can be seen above the model is easily able to pick the incomplete character name (\"Juli\" from Julian or \"Mr. Rol\" for \"Mr. Roland\" and is able to complete the text starting with completing the name which is quite extra ordinary ability for a small model which has been trained for ~5 minutes on a piece of text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
